config._config
config._config['plugins']
config._config['plugins:']
config._config['plugin:']
config._config['plugin:'].keys()
import os
from evaluation_system.api import plugin_manager
plugin_manager.PLUGIN_ENV
os.environ[plugin_manager.PLUGIN_ENV]
plugin_manager.getPlugins()
plugin_manager.getPlugins().keys()
name_width = 0
for key in pm.getPlugins():
                    name_width = max(name_width, len(key)
)
import evaluation_system.api.plugin_manager as pm
import textwrap
name_width=0
for key in pm.getPlugins():
                    name_width = max(name_width, len(key))
offset = name_width + 2
for key, plugin in sorted(pm.getPlugins().items()):
                    lines = textwrap.wrap('%s' % plugin['description'], env['columns'] - offset)
                    if not lines: lines = ['No description.']
                    if len(lines) > 1:
                        #multiline
                        print '%s: %s' % (plugin['name'], lines[0] + '\n' + ' '*offset +('\n' + ' '*offset).join(lines[1:]))
                    else:
                        print '%s: %s' % (plugin['name'], lines[0])
env = {'columns':80}
for key, plugin in sorted(pm.getPlugins().items()):
                    lines = textwrap.wrap('%s' % plugin['description'], env['columns'] - offset)
                    if not lines: lines = ['No description.']
                    if len(lines) > 1:
                        #multiline
                        print '%s: %s' % (plugin['name'], lines[0] + '\n' + ' '*offset +('\n' + ' '*offset).join(lines[1:]))
                    else:
                        print '%s: %s' % (plugin['name'], lines[0])
from evaluation_system.model.db import HistoryEntry
from evaluation_system.misc.utils import find_similar_words
pm.getPlugins()
import sys
sys.path
[p for p in sys.path if 'k204' in p
]
sys.path.append('/miklip/scratch/k204198/evaluation_system/bin')
import analyze
def loadlib(module_filepath):
    """Loads a module from a file not ending in .py"""
    #Try to tell python not to write these compiled files to disk
    import sys
    sys.dont_write_bytecode = True
    
    import imp
    
    py_source_open_mode = "U"
    py_source_description = (".py", py_source_open_mode, imp.PY_SOURCE)
    
    module_name = os.path.basename(module_filepath)
    with open(module_filepath, py_source_open_mode) as module_file:
        return imp.load_module(
                module_name, module_file, module_filepath, py_source_description)
a = loadlib('/miklip/scratch/k204198/evaluation_system/bin/analyze')
a.main("--help".split())
a.main("--list".split())
import test
test.MyPlugin()
m = test.MyPlugin()
m.getHelp()
print m.getHelp()
'123456789'split('3')
'123456789'.split('3')
'123456789'.split('5')
'123456789'.split('56')
from evaluation_system.model.solr_core load SolrCore
from evaluation_system.model.solr_core import SolrCore
s = SolrCore()
s.core
s.delete('data_type:baseline1')
rom evaluation_system.model.solr_core import SolrCore
from evaluation_system.model.solr_core import SolrCore
s = SolrCore() # points to the core named "files"
s.dump()
s.dump(batch_size=100000)
s = SolrCore('files2') # points to the core named "files"
s.core
s.delete('*')
s.get_json('')
s.echo
s.echo = True
s.get_json('')
s.get_json('q=*&rows=0')
s.get_json('?q=*&rows=0')
s.get_json('select?q=*&rows=0')
s2 = s
s = SolrCore()
s.get_json('select?q=*&rows=0')
s2.get_json('select?q=*&rows=0')
s2.load()
s2.load(batch_size=100000)
s2.get_json('select?q=*&rows=0')
s.get_json('select?q=*&rows=0')
help(s2.load)
1190*2+2
a=dict(a=1)
a
a.update(dict(b=2))
a
from evaluation_system.model.solr import SolrFileSearch
from evaluation_system.model.solr import SolrFindFiles
s = SolrFindFiles()
s.facets()
s.facets('time_frequency')
help(s.facets)
v=s.facets(latest_Version=False)
v
v=s.facets(latest_version=False)
v
s2 = SolrFindFiles('files2')
v2=s2.facets(latest_version=False)
s2.solr
s2.solr.core
s1.search(rows=1)
s.search(rows=1)
s.search(rows=1).next
s.search(rows=1).next()
s.search(latest_version=False, rows=1).next()
{}.put
{}.add
d = {}
a.update(dict(q='*'))
d.update(dict(q='*'))
d
from evaluation_system.model.solr import SolrFileSearch
import evaluation_system.model.solr
from evaluation_system.model.solr import SolrFileSearch
from evaluation_system.model.solr import SolrFindFiles
s = SolrFindFiles()
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'})
g=s.search(**{'variable': 'tas', 'data_type': 'reanalysis'})
g.next()
s.solr.echo=True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
s.solr
s.solr.echo
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr; reload(evaluation_system.model.solr);import evaluation_system.model.solr_core;reload(import evaluation_system.model.solr_core);from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
import evaluation_system.model.solr; reload(evaluation_system.model.solr); import evaluation_system.model.solr_core; reload(evaluation_system.model.solr_core); from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr_core; reload(evaluation_system.model.solr_core); import evaluation_system.model.solr; reload(evaluation_system.model.solr); from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr_core; reload(evaluation_system.model.solr_core); import evaluation_system.model.solr; reload(evaluation_system.model.solr); from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr_core; reload(evaluation_system.model.solr_core); import evaluation_system.model.solr; reload(evaluation_system.model.solr); from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s.search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
s._search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
import evaluation_system.model.solr_core; reload(evaluation_system.model.solr_core); import evaluation_system.model.solr; reload(evaluation_system.model.solr); from evaluation_system.model.solr import SolrFindFiles; s = SolrFindFiles(); s.solr.echo = True
s._search(**{'variable': 'tas', 'data_type': 'reanalysis'}).next()
from evaluation_system.model.file import DRSFile
import evaluation_system
from evaluation_system.model.file import DRSFile
fs = '/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/r1i1p1/sfcWind_6hrPlev_reanalysis_ERAINT-T255_r1i1p1_2011010100-2011123118.nc'
DRSFile.from_parh(fs)
DRSFile.from_path(fs)
drs = DRSFile.from_path(fs)
drs.to_dataset()
self = drs
def to_dataset(self, versioned=False):
        """creates the dataset to which this file is part of out of the DRS information.
:param versioned: If the dataset should contain inforation about the version. Note that not
                  all DRS structures are versioned, so in those cases where there is just no
                  version information this makes no difference.
:type versioned: bool"""
        result = []
        if versioned:
            if not self.is_versioned():
                raise Exception('%s is not versioned!' % self.drs_structure)
            iter_parts = self.getDrsStructure()['parts_versioned_dataset']
        else:  
            iter_parts = self.getDrsStructure()['parts_dataset']
            
        for key in iter_parts:
            if key in self.dict['parts']:
                result.append(self.dict['parts'][key])
            elif key in self.getDrsStructure()['defaults']:
                result.append(self.getDrsStructure()['defaults'][key])
        return '.'.join(result)
to_dataset(drs)
ds = to_dataset(drs)
drs.drs_structure
drs.getDrsStructure()
drs.getDrsStructure()['root_dir']
'/'.join([drs.getDrsStructure()['root_dir']] + ds.split('.'))
import os
os.path.isdir('/'.join([drs.getDrsStructure()['root_dir']] + ds.split('.')))
fs
fs[len(drs.getDrsStructure()['root_dir']:]
fs[len(drs.getDrsStructure()['root_dir']):]
fs[len(drs.getDrsStructure()['root_dir'])+1:]
fs[len(drs.getDrsStructure()['root_dir'])+1:].split('/')
fs[len(drs.getDrsStructure()['root_dir'])+1:].split('/')[:-1]
fs[len(drs.getDrsStructure()['root_dir'])+1:].split('/')[:-3]
def to_dataset(self, versioned=False, to_path=False):
        """creates the dataset to which this file is part of out of the DRS information.
:param versioned: If the dataset should contain inforation about the version. Note that not
                  all DRS structures are versioned, so in those cases where there is just no
                  version information this makes no difference.
:type versioned: bool
:param to_path: if true return the path to the dataset, otherwise returns the dataset identifier."""
        result = []
        if versioned:
            if not self.is_versioned():
                raise Exception('%s is not versioned!' % self.drs_structure)
            iter_parts = self.getDrsStructure()['parts_versioned_dataset']
        else:  
            iter_parts = self.getDrsStructure()['parts_dataset']
            
        for key in iter_parts:
            if key in self.dict['parts']:
                result.append(self.dict['parts'][key])
            elif key in self.getDrsStructure()['defaults']:
                result.append(self.getDrsStructure()['defaults'][key])
        if to_path:
            return self.getDrsStructure()['root_dir'] + '/' + '/'.join(result)
        else:
            return '.'.join(result)
to_dataset(drs)
to_dataset(drs, to_path)
to_dataset(drs, to_path=True)
fs = '/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/r1i1p1/sfcWind_6hrPlev_reanalysis_ERAINT-T255_r1i1p1_2011010100-2011123118.nc'
from evaluation_system.model.file import DRSFile
DRSFile.from_path(fs).to_dataset()
DRSFile.from_path(fs).to_dataset(to_path=True)
import os
os.path.isdir(DRSFile.from_path(fs).to_dataset(to_path=True)
)
'/a'.splti(
'/a'.split('/')
'/a/'.split('/')
[i for i in ''.split('/') if i]
[i for i in '/'.split('/') if i]
[i for i in '/a'.split('/') if i]
[i for i in '/a/b'.split('/') if i]
[i for i in '/a/b/'.split('/') if i]
'*'*3
'/'.join('*'*3)
'/'.join('*'*0)
from evaluation_system.model.file import DRSFile
fs = '/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/r1i1p1/sfcWind_6hrPlev_reanalysis_ERAINT-T255_r1i1p1_2011010100-2011123118.nc'
DRSFile.from_path(fs).to_dataset(to_path=True)
DRSFile.from_path(fs)
import evaluation_system.model.file; reload(evaluation_system.model.file); from evaluation_system.model.file import DRSFile
fs = '/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/r1i1p1/sfcWind_6hrPlev_reanalysis_ERAINT-T255_r1i1p1_2011010100-2011123118.nc'
DRSFile.from_path(fs).to_dataset(to_path=True)
start_path='/miklip/integration/data4miklip/reanalysis/ECMWF/'
structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
structures
structure = DRSFile._getDrsStructure(structures[0])
structure
elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
elements
path_elements = [i for i in start_path.split('/') if i]
missing_elements = elements - len(path_elements)
missing_elements
for drs_structure in structures:
    structure = DRSFile._getDrsStructure(drs_structure)
    elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
    missing_elements = elements - len(path_elements)
    if missing_elements >= 0:
        glob_str = start_path + '/' + '/'.join('*' * missing_elements)
    else:
        glob_str = '/'.join(path_elements[:missing_elements]
for drs_structure in structures:
    structure = DRSFile._getDrsStructure(drs_structure)
    elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
    missing_elements = elements - len(path_elements)
    if missing_elements >= 0:
        glob_str = start_path + '/' + '/'.join('*' * missing_elements)
    else:
        glob_str = '/'.join(path_elements[:missing_elements])
glob_str
import glob
g = glob.iglob(glob_str)
g
g.next()
def iterate_datasets(start_path = '/'):
    import glob
    
    structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob.iglob(glob_str):
def iterate_datasets(start_path = '/'):
    import glob
    
    structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob.iglob(glob_str)
iterate_datasets()
def find_structure_in_path(file_path, allow_multiples=False):
        structures = []
        for path_prefix, st_type in DRSFile._get_structure_prefix_map().items():
            if path_prefix.startswith(file_path):
                if allow_multiples:
                    structures.append(st_type)
                else:
                    return st_type
        if not structures:                                                                
            raise Exception("No DRS structure found in %s." % file_path)
        else:
            return structures
find_structure_in_path('/')
find_structure_in_path('/', allow_multiples=True)
find_structure_in_path('/as', allow_multiples=True)
find_structure_in_path('/miklip', allow_multiples=True)
find_structure_in_path('/miklip/', allow_multiples=True)
find_structure_in_path('/miklip/integration/data4miklip', allow_multiples=True)
find_structure_in_path('/miklip/integration/data4miklip/model', allow_multiples=True)
import evaluation_system.model.file; reload(evaluation_system.model.file); from evaluation_system.model.file import DRSFile
fs = '/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/r1i1p1/sfcWind_6hrPlev_reanalysis_ERAINT-T255_r1i1p1_2011010100-2011123118.nc'
DRSFile.from_path(fs).to_dataset(to_path=True)
def iterate_datasets(start_path = '/'):
    import glob
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob.iglob(glob_str)
iterate_datasets()
iterate_datasets().next()
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            missing_elements > len(len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob.iglob(glob_str)def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob.iglob(glob_str)
iterate_datasets()
iterate_datasets().next()
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir'] + '/' + '/'.join('*' * missing_elements)
            else:
                glob_str = start_path + '/' + '/'.join('*' * missing_elements)
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        return glob_str
iterate_datasets()
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = structure['root_dir']
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elements
iterate_datasets()
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = prefixes[drs_structure]
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elementsdef iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = prefixes[drs_structure]
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elements
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        elements = len(structure['parts_dataset']) + len([i for i in structure['root_dir'].split('/') if i])
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = prefixes[drs_structure]
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elements
iterate_datasets()
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/')
iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip')
iterate_datasets('/miklip/integration')
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        root_elem = len([i for i in structure['root_dir'].split('/') if i])
        parts_elem = len(structure['parts_dataset'])
        elements = root_elem + parts_elem
        
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = prefixes[drs_structure]
                missing_elements = elements - len(prefixes[drs_structure].split('/'))
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elements
iterate_datasets()
iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip')
iterate_datasets('/miklip/integration/')
iterate_datasets('/miklip/integration')
iterate_datasets('/miklip/')
iterate_datasets('/miklip/integration/data4miklip')
iterate_datasets()
fs
drs
DRSFile.from_path(fs)
DRSFile.from_path(fs)['parts']
DRSFile.from_path(fs)
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/asd')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/asd/sss/a')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/asd/sss')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind/asd')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/sfcWind')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr/atmos')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/ERAINT-T255/6hr')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/IFS/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis')
iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip')
iterate_datasets('/miklip/integration')
prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
prefixes
prefixes['observations']
DRSFile.DRS_STRUCTURE['observations']['parts_dataset']
len(DRSFile.DRS_STRUCTURE['observations']['parts_dataset'])
len(DRSFile.DRS_STRUCTURE['observations']['root'])
len(DRSFile.DRS_STRUCTURE['observations']['root_dir'])
len(DRSFile.DRS_STRUCTURE['observations']['root_dir'].split('/'))
len([i for i in DRSFile.DRS_STRUCTURE['observations']['root_dir'].split('/') if i])
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        root_elem = len([i for i in structure['root_dir'].split('/') if i])
        parts_elem = len(structure['parts_dataset'])
        elements = root_elem + parts_elem
        
        missing_elements = elements - len(path_elements)
        if missing_elements >= 0:
            if missing_elements > len(structure['parts_dataset']):
                #given startpath is less than required for deininf structure, 
                #dismiss that info completely
                glob_str = prefixes[drs_structure]
                missing_elements = elements - len([i for i in prefixes[drs_structure].split('/') if i])
            else:
                glob_str = start_path
        else:
            glob_str = '/'.join(path_elements[:missing_elements])
        
        print glob_str, missing_elements
iterate_datasets()
iterate_datasets()iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis')
iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis')
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    #now check all passing structures
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        
        root_elem = len([i for i in structure['root_dir'].split('/') if i])
        prefix_elem = len([i for i in prefixes[drs_structure].split('/') if i])
        parts_elem = len(structure['parts_dataset'])
        elements = root_elem + parts_elem
        
        if len(path_elements) > prefix_elem:
            #dataset at least partially selected
            path = start_path
            missing_levels = elements - len(path_elements)
        else:
            #all datasets
            path = prefixes[drs_structure]
            missing_levels = elements - prefix_elem
        
        print path, missing_levels
iterate_datasets()
iterate_datasets('/miklip/integration/data4miklip/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')
import os
os.walk('.')
g = os.walk('.')
g.next()
root,dirs,files = g.next()
dris
dirs
dirs = []
root,dirs,files = g.next(); print root,dirs,files
os.listdir('.')
root,dirs,files = g.next()
root,dirs,files = g.next(); print root,dirs,files
del dirs[:]
root,dirs,files = g.next(); print root,dirs,files
del dirs[:]
root,dirs,files = g.next(); print root,dirs,files
del dirs[:]
root,dirs,files = g.next(); print root,dirs,files
del dirs[:]
root,dirs,files = g.next(); print root,dirs,files
del dirs[:]
root,dirs,files = g.next(); print root,dirs,files
for root,dirs,files in os.walk('.'):
 print files
 del dirs[:]
for root,dirs,files in os.walk('.'):
 print files
def iterate_dirs(parent_dir, depth):
    if depth <= 0:
        return parent_dir
    for root, dirs, files in os.walk(whatever):
        if root.count(os.sep) >= depth:
            for d in dirs:
                yield os.path.join(root, d)
            del dirs[:]
def iterate_dirs(parent_dir, depth):
    for root, dirs, files in os.walk(parent_dir):
        if root.count(os.sep) >= depth:
            for d in dirs:
                yield os.path.join(root, d)
            del dirs[:]
iterate_dirs('.')
iterate_dirs('.', 0)
iterate_dirs('.', 0).next()
[d for d in iterate_dirs('.', 0)]
[d for d in iterate_dirs('.', 1)]
[d for d in iterate_dirs('.', 2)]
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    #now check all passing structures
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        
        root_elem = len([i for i in structure['root_dir'].split('/') if i])
        prefix_elem = len([i for i in prefixes[drs_structure].split('/') if i])
        parts_elem = len(structure['parts_dataset'])
        elements = root_elem + parts_elem
        
        if len(path_elements) > prefix_elem:
            #dataset at least partially selected
            path = start_path
            missing_levels = elements - len(path_elements)
        else:
            #all datasets
            path = prefixes[drs_structure]
            missing_levels = elements - prefix_elem
        
        for root, dirs, files in os.walk(path):
            if root.count(os.sep) >= missing_levels:
                for d in dirs:
                    yield os.path.join(root, d)
                del dirs[:]
g = iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')
g.next()
def iterate_datasets(start_path = '/'):
    import glob
    
    #clean path elements
    path_elements = [i for i in start_path.split('/') if i]
    
    try:    
        structures = DRSFile.find_structure_from_path(start_path, allow_multiples=True)
    except:
        #no passing structure... maybe is a subtype
        structures = DRSFile.find_structure_in_path(start_path, allow_multiples=True)
    
    #get all prefixes in reversed mapping order structure->prefix    
    prefixes = dict((v,k) for k,v in DRSFile._get_structure_prefix_map().items())
    
    #now check all passing structures
    for drs_structure in structures:
        structure = DRSFile._getDrsStructure(drs_structure)
        
        root_elem = len([i for i in structure['root_dir'].split('/') if i])
        prefix_elem = len([i for i in prefixes[drs_structure].split('/') if i])
        parts_elem = len(structure['parts_dataset'])
        elements = root_elem + parts_elem
        
        if len(path_elements) > prefix_elem:
            #dataset at least partially selected
            path = start_path
            missing_levels = elements - len(path_elements)
        else:
            #all datasets
            path = prefixes[drs_structure]
            missing_levels = elements - prefix_elem
        
        for root, dirs, files in os.walk(path):
            if root.count(os.sep) >= elements:
                for d in dirs:
                    yield os.path.join(root, d)
                del dirs[:]
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')
iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/').next()
[d for d in iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')]
ds = [d for d in iterate_datasets('/miklip/integration/data4miklip/reanalysis/ECMWF/')]; print '%s\n...Total: %s' % ('\n'.join(ds[:10]),len(ds))
ds = [d for d in iterate_datasets('/miklip/integration/data4miklip/reanalysis/')]; print '%s\n...Total: %s' % ('\n'.join(ds[:10]),len(ds))
ds = [d for d in iterate_datasets('/miklip/integration/data4miklip/observations/')]; print '%s\n...Total: %s' % ('\n'.join(ds[:10]),len(ds))
'a'.startswith
'a'.start_swith
'a'.startswith('b')
'a'.startswith('basdasd')
os
g = os.walk('.')
g.next()
dirs = ['.']
g.next()
dirs = ['.']
g.next()
dirs[:] = ['.']
g.next()
dirs[:] = ['.']
g.next()
for r, d, f in os.walk('.'):
    d [:] = sorted(d)
for r, d, f in os.walk('.'):
    d [:] = sorted(d)
    print r
for r, d, f in os.walk('.'):
    #d [:] = sorted(d)
    print r
for r, d, f in os.walk('.'):
    d.sort()
    print r
[].sort
help([].sort)
for r, d, f in os.walk('.'):
    d.sort(reverse=True)
    print r
500/60
4194304/1024
4194304/1024/1024
4194304/1024.0/1024
4194304/1024.0/1024.0
4194304.0/1024.0/1024.0
